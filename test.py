# æ–‡ä»¶å: run_test.py
# ä½ç½®: /root/shared-nvme/PoinTr-baseline-c/run_test.py
import torch
import torch.nn as nn
import sys
import types
import time
from collections import OrderedDict

# --- æ ¸å¿ƒé€»è¾‘ï¼šä»æ¨¡å‹æ–‡ä»¶ä¸­å¯¼å…¥æˆ‘ä»¬æƒ³è¦æµ‹è¯•çš„ç±» ---
# è¿™æ ·å¯¼å…¥å¯ä»¥ç¡®ä¿æ¨¡å‹ä»£ç åªè¢«æ‰§è¡Œä¸€æ¬¡ï¼Œé¿å…äº†é‡å¤æ³¨å†Œçš„é—®é¢˜ã€‚
from models.DiffSymm_refine import DiffSymm_refine
# --- æ¨¡æ‹Ÿå’Œå‡†å¤‡å·¥ä½œ (ä¸ä¹‹å‰ç›¸åŒ) ---
from extensions.chamfer_dist import ChamferDistanceL1

print("--- å‡†å¤‡æµ‹è¯•ç¯å¢ƒ: æ­£åœ¨æ¨¡æ‹Ÿä¾èµ–é¡¹... ---")

# 1. æ¨¡æ‹Ÿ (Mock) MODELS æ³¨å†Œå™¨å’Œæ„å»ºå™¨
#    è™½ç„¶æˆ‘ä»¬ä¸å†æœ‰é‡å¤æ³¨å†Œçš„é—®é¢˜ï¼Œä½†åœ¨ç‹¬ç«‹æµ‹è¯•æ—¶ä»éœ€æ¨¡æ‹Ÿè¿™ä¸ªæ¡†æ¶ä¾èµ–ã€‚
class MockModelsRegistry:
    def build(self, config):
        print(f"  - (Mock) æ­£åœ¨æ„å»º base_modelï¼Œä½¿ç”¨ nn.Identity() ä½œä¸ºå ä½ç¬¦ã€‚")
        return nn.Identity()
    def register_module(self):
        # åœ¨è¿™ä¸ªæµ‹è¯•æ–‡ä»¶ä¸­ï¼Œè¿™ä¸ªè£…é¥°å™¨ä»€ä¹ˆéƒ½ä¸ç”¨åš
        return lambda x: x

# ç”±äº DiffSymm_refine å·²ç»å¯¼å…¥å¹¶ä½¿ç”¨äº†åŸå§‹çš„ MODELS å¯¹è±¡,
# æˆ‘ä»¬éœ€è¦ç”¨ monkey-patch çš„æ–¹å¼æ›¿æ¢æ‰å®ƒï¼Œä»¥ä¾¿æ¨¡å‹èƒ½æˆåŠŸåˆå§‹åŒ–ã€‚
import models.DiffSymm_refine as model_file
model_file.MODELS = MockModelsRegistry()

# 2. å®šä¹‰ä¸€ä¸ªèƒ½åŒæ—¶æ”¯æŒ .å±æ€§ å’Œ .get() æ–¹æ³•çš„ MockConfig ç±»
class MockConfig(dict):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.__dict__ = self

print("--- ä¾èµ–é¡¹æ¨¡æ‹Ÿå®Œæˆã€‚å¼€å§‹æµ‹è¯•æ¨¡å‹... ---")

# 3. åˆ›å»ºé…ç½®å¯¹è±¡
config = MockConfig({
    'up_factors': '2,2,4',
    'num_proxy_steps': 5,
    'use_proxy_refiner': False,
    'pretrain': None,
    'base_model': {},
    'diffusion_cfg': {
        'training_mode': 'standard',
        'beta_schedule': 'linear',
        'ddim_num_steps': 50,
        'ddim_discretize': 'uniform',
        'ddim_eta': 0.0,
    }
})

# --- è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—æ¨¡å‹å‚æ•°é‡ ---
def count_parameters(model):
    """è®¡ç®—æ¨¡å‹çš„å‚æ•°é‡"""
    total_params = 0
    trainable_params = 0
    
    param_details = {}
    
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        
        if param.requires_grad:
            trainable_params += param_count
            
        # è®°å½•æ¯ä¸ªæ¨¡å—çš„å‚æ•°é‡
        module_name = name.split('.')[0] if '.' in name else name
        if module_name not in param_details:
            param_details[module_name] = {'total': 0, 'trainable': 0}
        
        param_details[module_name]['total'] += param_count
        if param.requires_grad:
            param_details[module_name]['trainable'] += param_count
    
    return total_params, trainable_params, param_details

def format_number(num):
    """æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤º"""
    if num >= 1e9:
        return f"{num/1e9:.2f}B"
    elif num >= 1e6:
        return f"{num/1e6:.2f}M"
    elif num >= 1e3:
        return f"{num/1e3:.2f}K"
    else:
        return str(num)

# --- è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—FLOPs ---
def calculate_flops(model, input_tensor):
    """ç®€å•çš„FLOPsè®¡ç®—"""
    flops = 0
    
    def flop_count_hook(module, input, output):
        nonlocal flops
        
        if isinstance(module, nn.Conv1d):
            # Conv1d: FLOPs = batch_size * output_length * kernel_size * in_channels * out_channels
            if hasattr(output, 'shape'):
                batch_size, out_channels, out_length = output.shape
                kernel_size = module.kernel_size[0]
                in_channels = module.in_channels
                flops += batch_size * out_length * kernel_size * in_channels * out_channels
                
        elif isinstance(module, nn.Linear):
            # Linear: FLOPs = batch_size * input_features * output_features
            if hasattr(output, 'shape') and len(output.shape) >= 2:
                batch_size = output.shape[0]
                output_features = output.shape[-1]
                input_features = module.in_features
                flops += batch_size * input_features * output_features
                
        elif isinstance(module, nn.BatchNorm1d):
            # BatchNorm: FLOPs = batch_size * num_features * length
            if hasattr(output, 'shape'):
                flops += output.numel()
    
    # æ³¨å†Œhook
    hooks = []
    for module in model.modules():
        if isinstance(module, (nn.Conv1d, nn.Linear, nn.BatchNorm1d)):
            hooks.append(module.register_forward_hook(flop_count_hook))
    
    # è¿è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­
    with torch.no_grad():
        model(input_tensor)
    
    # ç§»é™¤hook
    for hook in hooks:
        hook.remove()
    
    return flops

# --- è¾…åŠ©å‡½æ•°ï¼šæµ‹é‡æ¨ç†æ—¶é—´ ---
def measure_inference_time(model, input_tensor, num_runs=10):
    """æµ‹é‡æ¨ç†æ—¶é—´"""
    model.eval()
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(3):
            _ = model(input_tensor)
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # æµ‹é‡æ—¶é—´
    times = []
    for _ in range(num_runs):
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model(input_tensor)
            
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            
        end_time = time.time()
        times.append(end_time - start_time)
    
    avg_time = sum(times) / len(times)
    return avg_time, times

# --- æ­£å¼å¼€å§‹æµ‹è¯• ---
try:
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\n--- æ­£åœ¨è®¾å¤‡: {device} ä¸Šè¿è¡Œæµ‹è¯• ---")
    
    model = DiffSymm_refine(config=config).to(device)
    model.eval()
    print("âœ… æ¨¡å‹ DiffSymm_refine å®ä¾‹åŒ–æˆåŠŸã€‚")
    
    batch_size = 1
    num_points = 2048
    
    # åˆ›å»º (B, N, 3) å½¢çŠ¶çš„å¼ é‡
    dummy_point_cloud = torch.randn(batch_size, num_points, 3).to(device)
    print(f"âœ… åˆ›å»ºè™šæ‹Ÿè¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º: {dummy_point_cloud.shape}")
    
    # === è®¡ç®—æ¨¡å‹å‚æ•°é‡ ===
    print("\n" + "="*60)
    print("ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡")
    print("="*60)
    
    total_params, trainable_params, param_details = count_parameters(model)
    
    print(f"æ€»å‚æ•°é‡: {format_number(total_params)} ({total_params:,})")
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {format_number(trainable_params)} ({trainable_params:,})")
    print(f"ä¸å¯è®­ç»ƒå‚æ•°é‡: {format_number(total_params - trainable_params)} ({total_params - trainable_params:,})")
    
    print(f"\nå„æ¨¡å—å‚æ•°åˆ†å¸ƒ:")
    for module_name, counts in param_details.items():
        print(f"  - {module_name}: {format_number(counts['total'])} (å¯è®­ç»ƒ: {format_number(counts['trainable'])})")
    
    # === è®¡ç®—FLOPs ===
    print("\n" + "="*60)
    print("âš¡ è®¡ç®—å¤æ‚åº¦ç»Ÿè®¡")
    print("="*60)
    
    print("æ­£åœ¨è®¡ç®—FLOPs...")
    flops = calculate_flops(model, dummy_point_cloud)
    print(f"FLOPs: {format_number(flops)} ({flops:,})")
    
    # === æµ‹é‡æ¨ç†æ—¶é—´ ===
    print("\næ­£åœ¨æµ‹é‡æ¨ç†æ—¶é—´...")
    avg_time, times = measure_inference_time(model, dummy_point_cloud)
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms")
    print(f"æ¨ç†æ—¶é—´èŒƒå›´: {min(times)*1000:.2f} - {max(times)*1000:.2f} ms")
    
    # === å†…å­˜ä½¿ç”¨æƒ…å†µ ===
    if torch.cuda.is_available():
        print("\n" + "="*60)
        print("ğŸ’¾ GPUå†…å­˜ä½¿ç”¨æƒ…å†µ")
        print("="*60)
        
        memory_allocated = torch.cuda.memory_allocated(device) / 1024**2  # MB
        memory_reserved = torch.cuda.memory_reserved(device) / 1024**2   # MB
        
        print(f"GPUå†…å­˜åˆ†é…: {memory_allocated:.2f} MB")
        print(f"GPUå†…å­˜ä¿ç•™: {memory_reserved:.2f} MB")
    
    # === å‰å‘ä¼ æ’­æµ‹è¯• ===
    print("\n" + "="*60)
    print("ğŸš€ å‰å‘ä¼ æ’­æµ‹è¯•")
    print("="*60)
    
    with torch.no_grad():
        outputs = model(dummy_point_cloud)
    
    print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
    print("æ¨¡å‹è¿”å›ä¸€ä¸ªåŒ…å«å¤šä¸ªç‚¹äº‘çš„åˆ—è¡¨ï¼Œå…¶å½¢çŠ¶å¦‚ä¸‹:")
    
    output_names = ['coarse', 'fine1', 'fine2', 'fine3']
    for i, out_tensor in enumerate(outputs):
        print(f"  - è¾“å‡º {i} ({output_names[i]}): {out_tensor.shape}")
    
    # === æ€»ç»“ ===
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"âœ… æ¨¡å‹åç§°: DiffSymm_refine")
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {dummy_point_cloud.shape}")
    print(f"âœ… è¾“å‡ºæ•°é‡: {len(outputs)}")
    print(f"âœ… æ€»å‚æ•°é‡: {format_number(total_params)}")
    print(f"âœ… å¯è®­ç»ƒå‚æ•°: {format_number(trainable_params)}")
    print(f"âœ… è®¡ç®—å¤æ‚åº¦: {format_number(flops)} FLOPs")
    print(f"âœ… å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms")
    print(f"âœ… è®¾å¤‡: {device}")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼Œæ‰€æœ‰é—®é¢˜å·²è§£å†³ï¼")
    
except Exception as e:
    print("\n--- âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ ---")
    print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
    print(f"é”™è¯¯ä¿¡æ¯: {e}")
    import traceback
    traceback.print_exc()