optimizer: {
  type: AdamW,
  kwargs: {
    lr: 0.0002,
    weight_decay: 0.0005
  }
}

scheduler: {
  type: WarmUpCosLR,
  kwargs: {
    warmup_epoch: 20,
    max_epoch: 420,
    lr_max: 0.0002,
    lr_min: 0.00001
  }
}

dataset: {
  train: { _base_: cfgs/dataset_configs/PCN.yaml,
           others: {subset: 'train'} },
  val:   { _base_: cfgs/dataset_configs/PCN.yaml,
           others: {subset: 'test'} },
  test:  { _base_: cfgs/dataset_configs/PCN.yaml,
           others: {subset: 'test'} }
}

model: {
  NAME: Simba,
  pretrain: "./pretrained/SymmGT_best.pth",  # Pretrained weights from stage I
  base_model: {
    NAME: SymmGT,
    up_factors: "2, 8",
    include_input: False,
    coarse_up_factor: 2
  },
  up_factors: "2,2,4",
  include_input: False,
  coarse_up_factor: 2,
  diffusion_cfg: {
    timesteps: 100,                      # Number of diffusion steps during training
    beta_schedule: "linear",             # Beta scheduling strategy
    linear_start: 0.001,
    linear_end: 0.05,
    parameterization: "eps",             # Parameterization strategy
    training_mode: 'proxy_generation',   # Training mode
    # DDIM-specific configurations
    ddim_num_steps: 25,                  # Number of DDIM steps during inference
    ddim_discretize: "uniform",          # Discretization method: uniform or quad
    ddim_eta: 0.0,                       # Randomness factor: 0 is deterministic, 1 is equivalent to DDPM
    ddim_clip_denoised: True,           # Whether to clip the predicted denoised output during DDIM sampling
    clip_denoised: True,                # Whether to clip during training
    v_posterior: 0.0,                   # Posterior variance tuning parameter
    cosine_s: 0.008                     # Cosine schedule parameter
  }
}

total_bs: 32
step_per_update: 1
max_epoch: 420
val_freq: 2
num_proxy_steps: 10
consider_metric: CDL1

# Training  Losses = ['44.4153', '8.2504', '0.0068']
#   → sparse_loss (CDL1), dense_loss (CDL1), mse_loss
# Validation Losses = ['37.4845', '9.5724', '6.5911', '0.1990']
#   → SparseLossL1, SparseLossL2, DenseLossL1, DenseLossL2
